{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression\n",
        "\n",
        "Generalization:-\n",
        "Given a training set and ability to correctly predict when observing the test set. Given D = { ($x_i, y_i$), i = 1,2...m }. Construct a model M & Loss Function which minimizes on different set of data.\n",
        "\n",
        "\n",
        "---\n",
        "MSE\n",
        "L = 1\\m $Œ£_{i=1}^{m} (\\hat{y_i} - y)^2$\n",
        "\n",
        "Least Square Solution to the regression  = $W^* = (X^TX)^{-1}X^Ty$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Gradient Decent\n",
        "$W^{(k+1)}$ = $W^k - ùîë Œ¥L/Œ¥w$ . where ùîë is the learning rate.\n",
        "Find the global minimum\n",
        "\n",
        "\n",
        "---\n",
        "Overfitting\n",
        "\n",
        "training loss = 0 while test loss is huge.\n",
        "Weights &  Meansquare error will not be smooth . It will be all over the place from lower bound to higher bound.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Regularlization\n",
        "\n",
        "$Œª$ is used to penalize the weights where the prediction was bad. This will have a smooth curve.\n",
        "\n",
        "---\n",
        "\n",
        "Binary Cross Entropy Function\n",
        "\n",
        "$||w||^2$ = $W_0^2 +W_1^2 + W_2^2 ....+ W_n^2 $\n",
        "\n",
        "$L (\\hat{y}, y) = -1/m Œ£_{i=1}^{m}[y_{i}logÃÇ(\\hat{y}_i) + ( 1- y_{i}) log(1-\\hat{y}_i)]$\n",
        "where the 2 classes are y=0 & y =1 .\n",
        "\n",
        "---\n",
        "Covariance Matrix\n",
        "\n",
        "Correaltion between features and dimension reduction\n"
      ],
      "metadata": {
        "id": "zt72i-cGeqHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c4YDpvphetA0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQiwOz_8rQ1Q"
      },
      "source": [
        "\n",
        "**Gadirent Decent** is convex function.\n",
        "1. Starting point is arbitrary point for us to evaluate the performance.\n",
        "2.  find the derivative (or slope), and from there, we can use a tangent line to observe the steepness of the slope.\n",
        "3. Slope will inform the updates to the parameters‚Äîi.e. the weights and bias.\n",
        "\n",
        "**Batch Gradient Decent**\n",
        "\n",
        "Batch gradient descent sums the error for each point in a training set, updating the model only after all training examples have been evaluated.\n",
        "`Computation efficiency, long processing time for large training datasets & lot of memory(SGD) produces a stable error gradient and convergence,`\n",
        "\n",
        "**Stochastic gradient descent (SGD)**\n",
        "\n",
        "(SGD) runs a training epoch for each example within the dataset and it updates each training example's parameters one at a time\n",
        "`less memory. more computatinally expensive.noisy gradients. high speed in finding global mimimum`\n",
        "\n",
        "**Mini-batch gradient descent (MBD)**\n",
        "\n",
        "(MBD)combines concepts from both batch gradient descent and stochastic gradient descent. It splits the training dataset into small batch sizes and performs updates on each of those batches\n",
        "`computational efficiency with speed of gradient decent`\n",
        "\n",
        "Function :-\n",
        "$$ \\nabla_\\mathbf w L_{CE} = \\sum_{i=1}^m (\\hat{y}_i - y_i)x_i$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CART  - Calssificatoin & Regression Tree"
      ],
      "metadata": {
        "id": "bguUj9I-enY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaboost\n",
        "\n",
        "1. Initialization of Weight $W_0(i) = 1/ m$\n",
        "2. for t = T - 1\n",
        "        a. Fit a predictor \\hat{y}(x_i) to the training data and calculate the weights error scale\n",
        "  $œµ_{t} = p ( ÃÇ{y} <> y ) = ‚àë_{i=_( \\hat{y} <> y )}^{m} W_{t(i)}$ / $‚àë_{i=1}^{m} W_{t(i)}$\n",
        "        Basically sum of weights which we go wrong / sum of overall weights\n",
        "\n",
        "        where T = # of iterations\n",
        "              t = iteratror index  \n",
        "\n",
        "        b. Calculate the signficance\n",
        "Œ±_t = $ùîë log ( 1 - œµ_{t} ) / œµ_{t}$\n",
        "            ùîë :- Learning rate\n",
        "\n",
        "        c. Adjust the weights\n",
        "$W_{t+1}(i) =  $\n",
        "        if  \\hat{y} == y , do not change anything else increase Weight by\n",
        "$W_{t}(i) = œµ^{a_t} $\n",
        "\n",
        "3. $‚àë_{t=1}^{T} {a_t}\\hat{y}_t(i)$  every committee is does have the same weight\n",
        "  "
      ],
      "metadata": {
        "id": "bB34SHKWLDeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regression Tree\n",
        "We take the input of the predictions ( target ) from the first run , take this as the new y ( target ) on the same x & repeat the same for each iteration or epoch. lets say n=3.\n",
        "Finally emsemble all the 3 trees and make a new prediction.\n",
        "We calculate the RMSE for each tree"
      ],
      "metadata": {
        "id": "ykoIkhbyK7aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  Adaboost | Gradient Boosting Regression Tree   |   |   |   |\n",
        "|---|---|---|---|---|\n",
        "| Weak learner: Model doing slightly better than random guessing  | Sequential correction of predecessor‚Äôs errors  |   |   |   |\n",
        "| Each predictor pays more attention to the instances wrongly predicted by its predecessor.  | Does not tweak the weights of training instances\n",
        "|Train an ensemble of predictors sequentially. | Fit each predictor is trained using its predecessor‚Äôs residual errors as labels\n",
        "|Each predictor is assigned a coefficient that depends on the predictor‚Äôs training error|Gradient Boosted Trees: a CART is used as a base learner.\n",
        "\n"
      ],
      "metadata": {
        "id": "lXxb4y4419Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Gradient Boosting | Stochastic Gradient Boosting |  |  |  |\n",
        "|---|---|---|---|---|\n",
        "| GB involves an exhaustive search procedure |  Each tree is trained on a random subset of rows of the training data.|\n",
        "|Each CART is trained to find the best split points and features.|The sampled instances (40%-80% of the training set) are sampled without replacement.|\n",
        "|May lead to CARTs using the same split points and maybe the same features.| Features are sampled (without replacement) when choosing split points. Adds more vairance |"
      ],
      "metadata": {
        "id": "ZuU9kWeT4Bkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension Reduction\n",
        "\n",
        "---\n",
        "Higher dimensions features ( such as pictures ) can be very spare. This make features training very slow. Dimension reduction is one way keep most of the information but reduce the dimension. Main Approaches\n",
        "1. Projection\n",
        "Most features are higly coorelated this lie with lower dmiension subspace of higher dimension space.\n",
        "2. Manifold Learning\n",
        "Swiss Roll is a 2D manifold , which is a 2D shape, that can be bent and twisted in higher dimension. d-dimension manifold is part of an n-dimension space ( d < n ).\n",
        "3. Dimension Reduction is $\\sqrt{# dimension}$\n",
        "---\n",
        "PCA\n",
        "Idea is to keep as variance & minimize the mean square distance between original dataset and projection on the new axis with dimension reduction\n",
        "\n",
        "\n",
        "---\n",
        "SVD\n",
        "\n",
        "X = [m * n] where m is # of rows , n # of features.\n",
        "1. X = $X-ùûµ_{x}$ <- Substract mean from all data points\n",
        "2. S = $X^{T}X$ <- calculate the covariance\n",
        "3. Take SVD(S) = U Œ£ $V^{T}$\n",
        "   where U = [ $U_{1}, U_{2} ... U_{n}]_{n x m}$ <- contain Eigen vectors of\n",
        "   $X^{T}X$\n",
        "4. Œ£. =\n",
        "       [$œÉ_{1}$       \n",
        "                 $œÉ_{2}$\n",
        "                           $œÉ_{3}$ ]\n",
        "positive singular values where $œÉ_{1}$  > $œÉ_{2}$  > $œÉ_{3}$\n",
        "\n",
        "5. V = $œÉ_{1}$ , $œÉ_{2}$  last say give 90% of the answers , we can start to ignore the lower levels."
      ],
      "metadata": {
        "id": "X9-w-q5fahZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering\n",
        "\n",
        "Take a data set  consisting of observations of a random n-dimensional Euclidean variable. Goal is to partition the data set into some number\n",
        " of clusters whose inter-point distances are small compared with the distances to points outside of the cluster.\n",
        " 1. Chose centriod $C^{(1)}$ randomly from the dataset\n",
        " 2. Take new centriod $C^{(i)}$ of instance $x^{(i)}$ with a probability  $D(x^{(i)})^2$ / $‚àë_{j=1}^{m}$ $D(x^{(j)})^2$ where $D(x^{(i)})$ is the distance between instance of $x^j$ and the closet centreiod which was already chosen.  \n",
        " 3. Repeat till K centriods are chosen.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " Limits\n",
        " 1. Cluster sizes need to be known\n",
        " 2. Does not behave well when cluster are of different sizes , dimensions densities"
      ],
      "metadata": {
        "id": "WiY4oynmYvh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "True Positive Rate / Recall = $True Positive / (True Positive + False Negative)$\n",
        "\n",
        "Precision = $True Positive / (True\\space Positive + False \\space Positive)$\n",
        "Trade between TP & FP\n",
        "\n",
        "ROC :-  Receiever Operating Characertics Curve . Between False Negative & False Positive.\n",
        "A Good Classifier will capaure all ground truths will do so with minimal or no flase positive."
      ],
      "metadata": {
        "id": "XnQWy1yJsDUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Logistic Regression ( Linear Model )\n",
        "```\n",
        "Uses Sigmidial Unit\n",
        "\n",
        "$\\hat{y}^{(i)} = \\sigma( w^{T} x(i)) $\n",
        "\n",
        "logistic Regression  $log ( P(y=1|x) ) / ( P(y=0|x) ) = b + œÉ (W^{T}X)$\n",
        "\n",
        "logistic regression estimates the probability of an event occurring, such as voted or didn‚Äôt vote, based on a given dataset of independent variables.\n",
        "outcome is a probability, the dependent variable is bounded between 0 and 1.\n",
        "A logit transformation is applied on the odds‚Äîthat is, the probability of success divided by the probability of failure.\n",
        "\n",
        "Logit(pi) = 1/(1+ exp(-pi))\n",
        "\n",
        "ln(pi/(1-pi)) = $Beta_0 + Beta_1*X_1 + ‚Ä¶ + B_k*K_k$\n",
        "\n"
      ],
      "metadata": {
        "id": "W-U3gcm8u4Ii"
      }
    }
  ]
}